{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25f458c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.27.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.3)\n"
     ]
    }
   ],
   "source": [
    "# Request is an elegant and simple HTTP library for Python, built for human beings\n",
    "# Request allows you to send HTTP/1.1 request extremely easily.\n",
    "# This is the library(request), which we need to access any webpages.\n",
    "! pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "106feff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will find the years which will include the years we want to view the data.ra\n",
    "# the range function in python is non-inclusive, which means will have year between 1991 to 2021\n",
    "# we will cvonvert the range into a list for the better performance. now we will have a list from 1991 to 2021.\n",
    "years = list(range(1991,2022))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fc0f3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are goimg to define the url we want to scrape\n",
    "# so in place of year, we put a {} curly brackets, which will help us to scrape data for each year.b\n",
    "url_start = \"https://www.basketball-reference.com/awards/awards_{}.html\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21804d15",
   "metadata": {},
   "source": [
    "# Downloading MVP Voting Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4d0dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are going to iterate through the year.\n",
    "#so in the following code it will replace those brackets with specific year, so now we are iterating \n",
    "# through all of the year.starting from 1991, 1992.... 2022\n",
    "# Now we will import the requset\n",
    "# so in requst.get(url), will loop through year + will download that year data(url), which is amazing\n",
    "# next inside the loop we will open up our folder,and it will open up a new file for each year in html.\n",
    "# and then will open it with the write mode,then in the newxt line we will write our data into that file\n",
    "import requests\n",
    "for year in years:\n",
    "    url = url_start.format(year)\n",
    "    data = requests.get(url)\n",
    "     \n",
    "    with open(\"mvps/{}.html\".format(year), \"w+\") as f:\n",
    "        f.write(data.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed8d37d",
   "metadata": {},
   "source": [
    "# PARSING THE VOTES TABLE WITH BEAUTIFULSOUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5deb7d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n"
     ]
    }
   ],
   "source": [
    "# we will extract only the data table from each html file\n",
    "# for parsing( extracting specific data from the html webpages) \n",
    "# the webpages we will install the \"beautifulsoup4\", this librery is used to parse the webpages.\n",
    "! pip install beautifulsoup4\n",
    "# now we are going to import the beautifulsoup as the installation has completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8fd3121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "277d9319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we do not have a w+ because we want to open it with read mode\n",
    "# Here we will work with a single file\n",
    "with open(\"mvps/1991.html\") as f:\n",
    "    page = f.read()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87737abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are going to parse the 1991 data using beautifulsoup.\n",
    "# Now we are going to initialize our beautiful soup, instance passing in the page and saying \n",
    "# we want to use the HTML Parser.\n",
    "# here we are using the parser class to extract data from the html page\n",
    "soup = BeautifulSoup(page, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8791309",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'decompose'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# we would like to get rid of the some of the stuff, we can check that from the inspector, and will remove certain stuff\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# the row \"over_header\" we actually don't need them, when we import the data, it will just create an extra header row\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# which we don't need,so that will remove that.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# so the decompose method help us to remove our overheader\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# in summary we find these elements in html and then we use decompose to remove those elements\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mover_header\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompose\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'decompose'"
     ]
    }
   ],
   "source": [
    "# we would like to get rid of the some of the stuff, we can check that from the inspector, and will remove certain stuff\n",
    "# the row \"over_header\" we actually don't need them, when we import the data, it will just create an extra header row\n",
    "# which we don't need,so that will remove that.\n",
    "# so to remove that we are going to find that specific row, in html we have \"tr\" tag which has class over_header\n",
    "# so we want to find tr which is table_row with the class over_head and we want to remove that specific overhead.\n",
    "# so the decompose method help us to remove our overheader\n",
    "# in summary we find these elements in html and then we use decompose to remove those elements\n",
    "soup.find('tr', class_=\"over_header\").decompose()# finding the specific row which we want to remove.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec49fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to parse only the table, and specific table has an unique id.\n",
    "mvp_table = soup.find(id=\"mvp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9cd89f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "html5lib not found, please install it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# coolest thing is pandas can read a HTML- table\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# as we have installed pandas, now we are going to read this with pandas\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# will convert mvp_table as string \u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m mvp_1991 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_html\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmvp_table\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\util\\_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    312\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    313\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    314\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    315\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(inspect\u001b[38;5;241m.\u001b[39mcurrentframe()),\n\u001b[0;32m    316\u001b[0m     )\n\u001b[1;32m--> 317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\html.py:1205\u001b[0m, in \u001b[0;36mread_html\u001b[1;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links)\u001b[0m\n\u001b[0;32m   1201\u001b[0m validate_header_arg(header)\n\u001b[0;32m   1203\u001b[0m io \u001b[38;5;241m=\u001b[39m stringify_path(io)\n\u001b[1;32m-> 1205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\html.py:982\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, **kwargs)\u001b[0m\n\u001b[0;32m    980\u001b[0m retained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m flav \u001b[38;5;129;01min\u001b[39;00m flavor:\n\u001b[1;32m--> 982\u001b[0m     parser \u001b[38;5;241m=\u001b[39m \u001b[43m_parser_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflav\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m     p \u001b[38;5;241m=\u001b[39m parser(io, compiled_match, attrs, encoding, displayed_only, extract_links)\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\html.py:931\u001b[0m, in \u001b[0;36m_parser_dispatch\u001b[1;34m(flavor)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flavor \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbs4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml5lib\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _HAS_HTML5LIB:\n\u001b[1;32m--> 931\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml5lib not found, please install it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _HAS_BS4:\n\u001b[0;32m    933\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBeautifulSoup4 (bs4) not found, please install it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: html5lib not found, please install it"
     ]
    }
   ],
   "source": [
    "# coolest thing is pandas can read a HTML- table\n",
    "# as we have installed pandas, now we are going to read this with pandas\n",
    "# will convert mvp_table as string \n",
    "import pandas as pd\n",
    "mvp_1991 = pd.read_html(str(mvp_table))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd3c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvp_1991"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9879e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [] # creating an empty list\n",
    "# now we are going to read with pandas for all the years\n",
    "for year in years:\n",
    "    with open(\"mvp/{}.html\".format(year)) as f:\n",
    "        page = f.read()\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    soup.find('tr', class_=\"over_header\").decompose()\n",
    "    mvp_table = soup.find(id = \"mvp\")\n",
    "    mvp = pd.read_html(str(mvp_table))[0]\n",
    "    mvp[\"Year\"] = year # creating a new colomn as year to know about the Year, which year each player statistics from\n",
    "    # important to keep in mind that while combining multiple list, we need to identity them with certain matrix(row or column)\n",
    "    dfs.append(mvp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b15b728",
   "metadata": {},
   "source": [
    "# Combining MVP Votes with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992b2b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will use pandas concat to combine all the dataframe into one\n",
    "mvps = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2820a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvps.head() # looking at first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e008bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvps.tail() # looking the last few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d0afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are going to store the dataframe into CSV file\n",
    "mvps.to_csv(\"mvps.csv\") # now we have the csv file in our mvps folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35766933",
   "metadata": {},
   "source": [
    "# Dowloading Player Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0fdcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to predic who will be the best player, for that we need stastistical data\n",
    "# we will not get the full data with this approach\n",
    "Player_stats_url = \"https://www.basketball-reference.com/leagues/NBA_{}_per_game.html\"\n",
    "url = player_stats_url.format(1991)\n",
    "data = request.get(url)\n",
    "with open (\"player/1991,html\", \"w+\") as f:\n",
    "    f.write(data.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6f8ed38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.7.2-py3-none-any.whl (6.3 MB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting exceptiongroup>=1.0.0rc9\n",
      "  Downloading exceptiongroup-1.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Collecting async-generator>=1.9\n",
      "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: outcome, h11, exceptiongroup, async-generator, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed async-generator-1.10 exceptiongroup-1.1.0 h11-0.14.0 outcome-1.2.0 selenium-4.7.2 trio-0.22.0 trio-websocket-0.9.2 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "# Using SELENIUM TO SCRAPE A JAVA-SCRIPT PAGE\n",
    "# WE NEED TO FIGURE OT TO USE A BROWSER TO GET THIS DATA\n",
    "# THIS IS A COMMON PROBLEM IN WEBSCRAPNG, IF THE CODE IS IN JAVA-SCRIPT, WE NEED TO USE BROWSER FOR GETTING THE FULL DATA\n",
    "# let's install selenium\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4525a0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we need to install a driver, we need to go to certain webpage to find the driver and download it\n",
    "# for mac user run the given command \"xattr -d com.apple.quatantine\", for chromedriver\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba24924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will initialise the chrome driver, and we need to specify where the driver is\n",
    "# once will hit run, we can see the supercool driver\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/vik/chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d7aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 1991\n",
    "url = player_stats_url.format(year)\n",
    "driver.get(url) # here we are telling driver go and get the url or rendered the url\n",
    "driver.execute_script(\"window.scrollTo(1,10000)\") #javascript\n",
    "time.sleep(2)\n",
    "html = driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb311cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so after ruuning our data in java script, we can see html\n",
    "with open(\"player/{}.html\".format(year), \"w+\") as f:\n",
    "    f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385329c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so now we will go ahead and do that with rest of the seasons\n",
    "# it will take some time as it will tell the browser to download the command \n",
    "for year in years:\n",
    "    url = player_stats_url.format(year)\n",
    "    driver.get(url) # here we are telling driver go and get the url or rendered the url\n",
    "    driver.execute_script(\"window.scrollTo(1,10000)\") #javascript\n",
    "    time.sleep(2)\n",
    "    html = driver.page_source \n",
    "with open(\"player/{}.html\".format(year), \"w+\") as f:\n",
    "    f.write(html)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeab22c",
   "metadata": {},
   "source": [
    "# Parsing The Stats With BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05ba0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 1991\n",
    "with open(\"player/{}.html\".format(year)) as f:\n",
    "    page = f.read()\n",
    "soup = BeautifulDoup(page, \"html.parser\")\n",
    "soup.find('tr', class_=\"thead\").decompose()\n",
    "player_table = soup.find(id = \"per_game_stats\")\n",
    "player = pd.read_html(str(player_table))[0]\n",
    "player[\"Year\"] = year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9dbee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will turn this into a loop\n",
    "df = []\n",
    "for year in years:\n",
    "    \n",
    "    with open(\"player/{}.html\".format(year)) as f:\n",
    "    page = f.read()\n",
    "    soup = BeautifulDoup(page, \"html.parser\")\n",
    "    soup.find('tr', class_=\"thead\").decompose()\n",
    "    player_table = soup.find(id = \"per_game_stats\")\n",
    "    player = pd.read_html(str(player_table))[0]\n",
    "    player[\"Year\"] = year\n",
    "    dfs.append(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9bfc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "player.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eda2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "players = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ab555",
   "metadata": {},
   "outputs": [],
   "source": [
    "players.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d8f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "players.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccef4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's write this to csv\n",
    "player.to_csv(\"players.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43cf0b3",
   "metadata": {},
   "source": [
    "# Downloading Team Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e22d891",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_stats_url = \"https://www.basketball-reference.com/leagues/NBA_{}_standings.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2740625d",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u010d' in position 188949: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mteam/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(year), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\encodings\\cp1252.py:19\u001b[0m, in \u001b[0;36mIncrementalEncoder.encode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\u010d' in position 188949: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "for year in years:\n",
    "    url = team_stats_url.format(year)\n",
    "    data = requests.get(url)\n",
    "    with open(\"team/{}.html\".format(year), \"w+\") as f:\n",
    "        f.write(data.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daed1168",
   "metadata": {},
   "source": [
    "# Parsing and Combining Team Data With BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9be21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for year in years:\n",
    "    with open(\"team/{}.html\".format(year)) as f:\n",
    "    page = f.read()\n",
    "    soup = BeautifulDoup(page, \"html.parser\")\n",
    "    soup.find('tr', class_=\"thead\").decompose()\n",
    "    team_table = soup.find(id = \"divs_standing_E\")\n",
    "    team = pd.read_html(str(team_table))[0]\n",
    "    team[\"Year\"] = year\n",
    "    team[\"Team\"] = team[\"Eastern Conference\"]\n",
    "     del team[\"Eastern Conference\"] \n",
    "    dfs.append(team)\n",
    "\n",
    "     soup = BeautifulDoup(page, \"html.parser\")\n",
    "    soup.find('tr', class_=\"thead\").decompose()\n",
    "     team_table = soup.find(id = \"divs_standing_E\")\n",
    "    team = pd.read_html(str(team_table))[0]\n",
    "    team[\"Year\"] = year\n",
    "    team[\"Team\"] = team[\"Western Conference\"]\n",
    "    del team[\"Western Conference\"]\n",
    "    dfs.append(team)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af492fc2",
   "metadata": {},
   "source": [
    "# Combining Team Stats With Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3229da49",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eccd63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams # so now we have parsed all the team data and put it into a single dataframe\n",
    "teams = teams.to_csv(\"teams.csv\")\n",
    "teams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
